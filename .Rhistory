p2hat_conditional[i] <- p1hat[i] + mean(heal2[heal1[1:n2] == 0]) * (1 - p1hat[i])
# 3rd time point
p3hat_naive[i] <- mean(heal3)
p3hat_intersection[i] <- p2hat_conditional[i] + mean((1 - heal2[1:n3]) & heal3)
p3hat_conditional[i] <- p2hat_conditional[i] + mean(heal3[heal2[1:n3] == 0]) * (1 - p2hat_conditional[i])
}
# Check for bias and variance
mean((p1hat - p1)^2)
mean((p2hat_naive - p2)^2)
# The two below are quite a bit better, with conditional seeming even slightly better; seems to be consistent
mean((p2hat_intersection - p2)^2)
mean((p2hat_conditional - p2)^2)
eeks = c(0, 2, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52)
ntotal = c(400, 382, 348, 287, 172, 136, 113, 92, 73, 55, 41, 35, 27, 19)
missing = c(1, 51, 10, 17, 17, 14, 20, 13, 11, 9, 11, 9, 7, 6, 0)
weeks = c(0, 2, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52)
ntotal = c(400, 382, 348, 287, 172, 136, 113, 92, 73, 55, 41, 35, 27, 19)
missing = c(1, 51, 10, 17, 17, 14, 20, 13, 11, 9, 11, 9, 7, 6, 0)
weeks = c(0, 2, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52)
ntotal = c(400, 382, 348, 287, 172, 136, 113, 92, 73, 55, 41, 35, 27, 19)
missing = c(1, 51, 10, 17, 17, 14, 20, 13, 11, 9, 11, 9, 7, 6, 0)
closed = c(1, 3, 9, 3, 7, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0)
closed_new = cumsum(closed)
closed_new/(ntotal-missing)
dim(closed_new)
?cumsum
weeks = c(0, 2, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52)
ntotal = c(400, 382, 348, 287, 172, 136, 113, 92, 73, 55, 41, 35, 27, 19)
missing = c(1, 51, 10, 17, 17, 14, 20, 13, 11, 9, 11, 9, 7, 6, 0)
closed = c(1, 3, 9, 3, 7, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0)
cumsum(closed)
length(cumsum(closed))
closed_new = cumsum(closed)
closed_new/ntotal
length(n_total)
length(ntotal)
weeks = c(0, 2, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52)
ntotal = c(400, 382, 348, 287, 226, 172, 136, 113, 92, 73, 55, 41, 35, 27, 19)
missing = c(1, 51, 10, 17, 17, 14, 20, 13, 11, 9, 11, 9, 7, 6, 0)
closed = c(1, 3, 9, 3, 7, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0)
closed_new = cumsum(closed)
closed_new/ntotal * 100
closed_new/(ntotal - missing) * 100
# look at 5-8 estimates
weeks[5:8]
0.4 * 5
23 + 10 + 6 + 10 + 8 + 9
23/66
10/66
9/66
8/66
6/66
35 + 15 + 9 + 15 + 12 + 14
0.35 * 12
library(tree)
library(ICLR)
install.packages("tree")
install.packages("ICLR")
citation("ICLR")
library(ICLR)
install.packages("ISLR")
library(ISLR)
citation("ISLR")
citation("tree")
?Carseats
library(tree) # there are also other libraries
library(ICLR) #From James et al., Introduction to Statistical Learning
library(tree) # there are also other libraries
library(ISLR) #From James et al., Introduction to Statistical Learning
attach(Carseats)
# Sales of child carsets at 400 multiple stores with 11 variables
# Binarize outcome for illustration
Carseats$High = ifesle(Sales <= 8, "No", "Yes")
# Binarize outcome for illustration
Carseats$High = ifelse(Sales <= 8, "No", "Yes")
?tree
?tree.control
summary(tree.carseats)
# Build a tree based on gini index
tree.carseats = tree(High ~ . - Sales, split = "gini", data = Carseats)
summary(tree.carseats)
library(tree) # there are also other libraries
library(ISLR) #From James et al., Introduction to Statistical Learning
attach(Carseats)
# Sales of child carsets at 400 multiple stores with 11 variables
# Binarize outcome for illustration
Carseats$High = ifelse(Sales <= 8, "No", "Yes")
# Build a tree based on gini index
tree.carseats = tree::tree(High ~ . - Sales, split = "gini", data = Carseats)
summary(tree.carseats)
# Build a tree based on gini index
tree.carseats = tree::tree(High ~ . -Sales, split = "gini", data = Carseats)
head(Carseats)
# Build a tree based on gini index
tree.carseats = tree::tree(High ~ . -Sales, data = Carseats)
# Build a tree based on gini index
tree.carseats = tree(High ~ .-Sales, data = Carseats)
library(tree) # there are also other libraries
library(ISLR) #From James et al., Introduction to Statistical Learning
# Binarize outcome for illustration
Carseats$High = ifelse(Carseats$Sales <= 8, 0, 1)
str(Carseats)
# Build a tree based on gini index
tree.carseats = tree(High ~ .-Sales, data = Carseats)
# Build a tree based on gini index
tree.carseats = tree(High ~ .-Sales, split = "gini", data = Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
newdata = apply(nhanes_matrix, 2, as.numeric)
##### Try birthweight instead #####
head(birthwt)
data(birthwt)
##### Try birthweight instead #####
library(MASS)
head(birthwt)
str(birthwt)
?tree.control
tree.birthwt = tree(low ~ age + lwt + race + smoke + ptl + ht + ui + ftv, data = birthwt, split = "gini", mincut = 10)
plot(tree.birthwt)
text(tree.birthwt, pretty = 0)
?birthwt
?plot.tree
tree.birthwt = tree(low ~ age + lwt + race + smoke + ptl + ht + ui + ftv, data = birthwt, split = "deviance", mincut = 10)
plot(tree.birthwt)
text(tree.birthwt, pretty = 0)
tree.birthwt = tree(low ~ age + lwt + race + smoke + ptl + ht + ui + ftv, data = birthwt, mincut = 10)
# Plot resulting tree
plot(tree.birthwt)
text(tree.birthwt, pretty = 0)
?tree.prube
?tree.prune
?prune.tree
?cv.tree
?prune.tree
?cv.tree
# Try to prune
cv.tree(tree.birthwt, , prune.tree)
cv.tree(tree.birthwt, , prune.tree)
?prune.tree
# For simplicity, evalute on train
test = birthwt
tree_pred = predict(tree_carseats, test, type = "class")
# For simplicity, evalute on train
test = birthwt
tree_pred = predict(tree.birthwt, test, type = "class")
?tree
?tree.control
?tree.predict
?predict.tree
str(tree.birthwt)
tree_pred = predict(tree.birthwt, test, type = "class")
tree_pred = predict(tree.birthwt, test)
tree_pred
tree_pred = ifelse(tree_pred_prob > thresh, 1, 0)
test = birthwt
tree_pred_prob = predict(tree.birthwt, test)
thresh = 0.8
tree_pred = ifelse(tree_pred_prob > thresh, 1, 0)
table(tree_pred, test$low)
# For simplicity, evalute on train
test = birthwt
tree_pred_prob = predict(tree.birthwt, test)
thresh = 0.7
tree_pred = ifelse(tree_pred_prob > thresh, 1, 0)
table(tree_pred, test$low)
?cv.tree
# Try to prune based on misclassification error rate (can also use deviance)
cv_birthwt = cv.tree(tree.birthwt, FUN = prune.misclass)
# Try to prune based on misclassification error rate (can also use deviance)
cv_birthwt = cv.tree(tree.birthwt, , prune.tree)
# Try to prune based on misclassification error rate (can also use deviance)
cv_birthwt = cv.tree(tree.birthwt, , prune.tree)
cv_birthwt = cv.tree(tree.birthwt, , prune.tree)
cv_birthwt
?prune.tree
?prune.tree
?cv.tree
# Smallest deviance for smallest tree
# Can also use new data to help prune
new.tree = prune.tree(tree.birthwt, k = 30)
plot(new.tree)
str(new.tree)
plot(new.tree)
# Smallest deviance for smallest tree
# Can also use new data to help prune
new.tree = prune.tree(tree.birthwt, k = 3)
plot(new.tree)
?prune.tree
# Smallest deviance for smallest tree
# Can also use new data to help prune
new.tree = prune.tree(tree.birthwt, k = 3)
str(new.tree)
data(fgl, package="MASS")
fgl.tr <- tree(type ~ ., fgl)
print(fgl.tr); plot(fgl.tr)
fgl.cv <- cv.tree(fgl.tr,, prune.tree)
for(i in 2:5)  fgl.cv$dev <- fgl.cv$dev +
cv.tree(fgl.tr,, prune.tree)$dev
fgl.cv$dev <- fgl.cv$dev/5
plot(fgl.cv)
fgl.cv <- cv.tree(fgl.tr,, prune.tree)
fgl.cv
# Smallest deviance for smallest tree
# Can also use new data to help prune
new.tree = prune.tree(tree.birthwt, best = 5)
plot(new.tree)
text(new.tree, pretty = 0)
(0.03 - 0.06 * 0.37)/(1-0.37)
(0.03 - 0.06 * 0.1)/(1-0.1)
library(iglu)
active_percent(example_data_5_subject)
?cv_glu
?gri
gri
?agp
names(all_metrics(example_data_5_subject,
metrics_to_include = "consensus_only"))
?sd_measures
?adrr
?ea1c
mage(example_data_1_subject)
mage(example_data_1_subject, plot = TRUE)
mage(example_data_5_subject %>% dplyr::filter(id == "Subject 2"), plot = TRUE)
mage(example_data_5_subject %>% dplyr::filter(id == "Subject 3"), plot = TRUE)
?sd_roc
?hist_roc
plot_lasagna(example_data_5_subject)
plot_lasagna
?plot_lasagn
?plot_lasagna
plot_lasagna(example_data_5_subject, lasagnatype = "subjectsorted")
plot_lasagna(example_data_5_subject, lasagnatype = "timesorted")
plot_lasagna_1subject(example_data_1_subject)
?plot_lasagna_1subject
plot_lasagna_1subject(example_data_1_subject, color_scheme = 'red-orange')
plot_lasagna_1subject(example_data_1_subject)
plot_lasagna_1subject(example_data_1_subject, type = "timesorted")
?plot_lasagna_1subject
plot_lasagna_1subject(example_data_1_subject, lasagnatype = "timesorted")
citation(iglu)
citation("iglu")
require("iglu")
source("~/Library/CloudStorage/GoogleDrive-irinagn@umich.edu/My Drive/Presentations/2025/Banff/SlidesWorkshop.R")
plot_glu(example_data_1_subject)
require("iglu")
require("dplyr")
library(iglu)
# Visualize example data in iglu
plot_glu(example_data_1_subject)
plot_glu(example_data_5_subject)
# Alternative lasagna visualization by subject
plot_lasagna_1subject(example_data_1_subject)
# Sorted by time to get average trends
plot_lasagna_1subject(example_data_1_subject,
lasagnatype = 'timesorted')
# Check % missing data
active_percent(example_data_5_subject)
# Mean
mean_glu(example_data_5_subject)
# GMI = 3.31 + 0.02392 \times \text{mean glucose}
gmi(example_data_5_subject)
# Time in range (TIR)
in_range_percent(example_data_5_subject,
target_ranges = list(c(70, 180)))
# Alternative from the plots, can adjust
plot_glu(example_data_5_subject, LLTR = 70, ULTR = 180)
# All glycemic ranges plot
plot_ranges(example_data_5_subject %>%
dplyr::filter(id == "Subject 5"))
agp(example_data_1_subject, daily = FALSE)
57 * 1.03
57 * 1.05
57 * 1.1
knitr::opts_chunk$set(echo = TRUE)
# Generate a sample of size 15 from Poisson distribution with mean 10
set.seed(123)
n <- 15
lambda <- 10
dataX <- rpois(n, lambda)
dataX
# Generate a sample of size 15 from Poisson distribution with mean 10
set.seed(123)
n <- 10
lambda <- 10
dataX <- rpois(n, lambda)
dataX
# Generate a sample of size 10 from Poisson distribution with mean 5
set.seed(123)
n <- 10
lambda <- 5
dataX <- rpois(n, lambda)
dataX
# Generate a sample of size 10 from Poisson distribution with mean 7
set.seed(123)
n <- 10
lambda <- 7
dataX <- rpois(n, lambda)
dataX
# Generate a sample of size 10 from Poisson distribution with mean 15
set.seed(123)
n <- 10
lambda <- 7
dataX <- rpois(n, lambda)
dataX
# Generate a sample of size 10 from Poisson distribution with mean 15
set.seed(123)
n <- 10
lambda <- 15
dataX <- rpois(n, lambda)
dataX
# Generate a sample of size 10 from Poisson distribution with mean 15
set.seed(123)
n <- 10
lambda <- 15
dataX <- rpois(n, lambda)
dataX
# Obtain lambda_hat using empirical mean
lambda_hat = mean(dataX)
lambda_hat
# do one boostrap replication
dataX_star <- sample(dataX, n, replace = TRUE)
# obtain lambda_star using empirical mean
lambda_star = mean(dataX_star)
# do one boostrap replication
dataX_star <- sample(dataX, n, replace = TRUE)
# obtain lambda_star using empirical mean
lambda_star = mean(dataX_star)
# do one boostrap replication
set.seed(53896)
dataX_star <- sample(dataX, n, replace = TRUE)
dataX_star
# obtain lambda_star using empirical mean
lambda_star = mean(dataX_star)
lambda_star
# do 1000 bootstrap replications
B = 1000
set.seed(24856)
lambda_star = rep(NA, B)
for (b in 1:B){
# sample data with replacement
dataX_star <- sample(dataX, n, replace = TRUE)
# estimate lambda_star
lambda_star[b] = mean(dataX_star)
}
# plot the histogram of lambda_star
hist(lambda_star, breaks = 30, main = "Bootstrap Distribution of Lambda", xlab = "Lambda Star", col = "lightblue")
# add vertical line for lambda_hat and true lambda
abline(v = lambda_hat, col = "red", lwd = 2)
abline(v = lambda, col = "blue", lwd = 2)
# estimate from original sample
lambda_hat
# bootstrap sample mean
mean(lambda_star)
# bias
mean(lambda_star) - lambda_hat
# bootstrap variance
var(lambda_star)
# large sample 95% CI
lambda_hat + c(-1, 1) * qnorm(0.975) * sqrt(var(lambda_star))
?choose
choose(2*10 - 1, 10)
# Consider bias of parametric bootsrap
bias = theta_starP - theta_hat
knitr::opts_chunk$set(echo = TRUE)
# Generate a sample of size 10 from Poisson distribution with mean 15
set.seed(123)
n <- 10
lambda <- 15
dataX <- rpois(n, lambda)
dataX
# Obtain lambda_hat using empirical mean
lambda_hat = mean(dataX)
lambda_hat
# do one boostrap replication
set.seed(53896)
dataX_star <- sample(dataX, n, replace = TRUE)
dataX_star
# obtain lambda_star using empirical mean
lambda_star = mean(dataX_star)
lambda_star
# do 1000 bootstrap replications
B = 1000
set.seed(24856)
lambda_star = rep(NA, B)
for (b in 1:B){
# sample data with replacement
dataX_star <- sample(dataX, n, replace = TRUE)
# estimate lambda_star
lambda_star[b] = mean(dataX_star)
}
# plot the histogram of lambda_star
hist(lambda_star, breaks = 30, main = "Bootstrap Distribution of Lambda",
xlab = "Lambda Star", col = "lightblue")
# add vertical line for lambda_hat and true lambda
abline(v = lambda_hat, col = "red", lwd = 2)
abline(v = lambda, col = "blue", lwd = 2)
# estimate from original sample
lambda_hat
# bootstrap sample mean
mean(lambda_star)
# bias
mean(lambda_star) - lambda_hat
# bootstrap variance
var(lambda_star)
# large sample 95% CI
lambda_hat + c(-1, 1) * qnorm(0.975) * sqrt(var(lambda_star))
# Use direct percentiles of lambda_star
# 95% CI
quantile(lambda_star, c(0.025, 0.975))
# do one parametric boostrap replication
set.seed(53896)
dataX_star <- rpois(n, lambda = lambda_hat)
dataX_star
# obtain lambda_star using empirical mean
lambda_star = mean(dataX_star)
lambda_star
# do 1000 parametric bootstrap replications
B = 1000
set.seed(24856)
lambda_star = rep(NA, B)
for (b in 1:B){
# sample data from parametric distribution with estimated parameter
dataX_star <- rpois(n, lambda = lambda_hat)
# estimate lambda_star
lambda_star[b] = mean(dataX_star)
}
# plot the histogram of lambda_star
hist(lambda_star, breaks = 30, main = "Parametric bootstrap Distribution of Lambda",
xlab = "Lambda Star", col = "lightblue")
# add vertical line for lambda_hat and true lambda
abline(v = lambda_hat, col = "red", lwd = 2)
abline(v = lambda, col = "blue", lwd = 2)
# estimate from original sample
lambda_hat
# bootstrap sample mean
mean(lambda_star)
# bias
mean(lambda_star) - lambda_hat
# bootstrap variance
var(lambda_star)
# large sample 95% CI
lambda_hat + c(-1, 1) * qnorm(0.975) * sqrt(var(lambda_star))
# Use direct percentiles of lambda_star
# 95% CI
quantile(lambda_star, c(0.025, 0.975))
# Use n = 30 to generate sample from uniform on 0, theta with theta = 2
n <- 30
theta <- 2
set.seed(532958)
dataX <- runif(n, 0, theta)
# Construct theta_hat as empirical maximum
theta_hat = max(dataX)
theta_hat
# Try both nonparametric and parametric bootstrap
B = 1000
theta_starP = rep(NA, B)
theta_starN = rep(NA, B)
for (b in 1:B){
# Sample with replacement
dataX_star <- sample(dataX, n, replace = TRUE)
# Estimate theta_starN as max
theta_starN[b] = max(dataX_star)
# Sample frrom parametric distribution
dataX_star <- runif(n, 0, theta_hat)
# Estimate theta_starP as max
theta_starP[b] = max(dataX_star)
}
# Create side by side histogram of theta_starN and thera_starP
par(mfrow = c(1, 2))
hist(theta_starN, breaks = 30, main = "Nonparametric Bootstrap",
xlab = "Theta Star", col = "lightblue")
# add vertical line for theta_hat and true theta
abline(v = theta_hat, col = "red", lwd = 2)
abline(v = theta, col = "blue", lwd = 2)
hist(theta_starP, breaks = 30, main = "Parametric Bootstrap",
xlab = "Theta Star", col = "lightblue")
# add vertical line for theta_hat and true theta
abline(v = theta_hat, col = "red", lwd = 2)
abline(v = theta, col = "blue", lwd = 2)
# Consider bias of parametric bootsrap
bias = theta_starP - theta_hat
hist(bias)
# Consider bias of parametric bootsrap
bias = theta_starP - theta_hat
hist(bias)
# Create 95% CI for theta by adjusting for bias with quantiles
theta_hat + quantiles(bias, c(0.025, 0.975))
# Consider bias of parametric bootsrap
bias = theta_starP - theta_hat
hist(bias)
# Create 95% CI for theta by adjusting for bias with quantiles
theta_hat + quantile(bias, c(0.025, 0.975))
# Consider bias of parametric bootsrap
bias = theta_starP - theta_hat
hist(bias)
# Create 95% CI for theta by adjusting for bias with quantiles
theta_hat - quantile(bias, c(0.025, 0.975))
# Consider bias of parametric bootsrap
bias = theta_starP - theta_hat
hist(bias)
# Create 95% CI for theta by adjusting for bias with quantiles
theta_hat - quantile(bias, c(0.075, 0.025))
# Consider bias of parametric bootsrap
bias = theta_starP - theta_hat
hist(bias)
# Create 95% CI for theta by adjusting for bias with quantiles
theta_hat - quantile(bias, c(0.975, 0.025))
library(tmvnsim)
citation("tmvnsim")
72 * 72
setwd("~/Documents/irinagain.github.io")
