cl <- makeCluster(nworkers)
registerDoParallel(cl)
# foreach loop
mse_beta <- foreach (i = 1:nrep) %dopar% {
# One instance of data generation (making X and Y)
X = matrix(rnorm(n * p, sd = 3), n, p)
Y = X %*% beta + rnorm(n)
# obtain least squares estimator
beta_LS = solve(crossprod(X), crossprod(X,Y))
# calculate estimation error
out = sum((beta_LS - beta)^2)
out
}
stopCluster(cl)
mse_beta
nrep = 5
# Set up the cluster
# Determine the number of workers I can use
nworkers = detectCores() - 1 # for my machine, to leave for other uses
cl <- makeCluster(nworkers)
registerDoParallel(cl)
# foreach loop
mse_beta <- foreach (i = 1:nrep) %dopar% {
# One instance of data generation (making X and Y)
X = matrix(rnorm(n * p, sd = 3), n, p)
Y = X %*% beta + rnorm(n)
# obtain least squares estimator
beta_LS = solve(crossprod(X), crossprod(X,Y))
# calculate estimation error
out = sum((beta_LS - beta)^2)
out
}
stopCluster(cl)
mse_beta
nrep = 5
# Set up the cluster
# Determine the number of workers I can use
nworkers = detectCores() - 1 # for my machine, to leave for other uses
cl <- makeCluster(nworkers)
registerDoParallel(cl)
# foreach loop
mse_beta <- foreach (i = 1:nrep) %dopar% {
# One instance of data generation (making X and Y)
X = matrix(rnorm(n * p, sd = 3), n, p)
Y = X %*% beta + rnorm(n)
# obtain least squares estimator
beta_LS = solve(crossprod(X), crossprod(X,Y))
# calculate estimation error
out = sum((beta_LS - beta)^2)
out
}
stopCluster(cl)
mse_beta
nrep = 5
# Set up the cluster
# Determine the number of workers I can use
nworkers = detectCores() - 1 # for my machine, to leave for other uses
cl <- makeCluster(nworkers)
registerDoParallel(cl)
output <- foreach (i = 1:nrep) %dopar% {
X = matrix(rnorm(n * p, sd = 3), n, p)
Y = X %*% beta + rnorm(n)
list(X = X, Y = Y)
}
stopCluster(cl)
length(output)
output[[1]]
names(output[[1]])
getwd()
save(output, file = "DataSim1.Rda")
load("DataSim1.Rda")
?load
nrep = 10
x = rep(NA, nrep)
for (i in 1:nrep){
x[i] = runif(1)
}
x
nrep = 10
x = rep(NA, nrep)
for (i in 1:nrep){
x[i] = runif(1)
}
x
nrep = 10
x = rep(NA, nrep)
for (i in 1:nrep){
x[i] = runif(1)
}
x
set.seed(2340387)
nrep = 10
x = rep(NA, nrep)
for (i in 1:nrep){
x[i] = runif(1)
}
x
set.seed(2340387)
nrep = 10
x = rep(NA, nrep)
for (i in 1:nrep){
x[i] = runif(1)
}
x
set.seed(2340387)
nrep = 10
x = rep(NA, nrep)
for (i in 1:nrep){
x[i] = runif(1)
}
x
set.seed(2340387)
nrep = 10
x = rep(NA, nrep)
for (i in 1:nrep){
x[i] = runif(1)
}
x
set.seed(2340387)
nrep = 10
x = rep(NA, nrep)
for (i in 1:nrep){
x[i] = runif(1)
}
x
set.seed(2340387)
nrep = 10
x = rep(NA, nrep)
for (i in 1:nrep){
x[i] = runif(1)
}
x
set.seed(61234)
nrep = 10
x = rep(NA, nrep)
for (i in 1:nrep){
x[i] = runif(1)
}
x
set.seed(61234)
nrep = 10
x = rep(NA, nrep)
for (i in 1:nrep){
x[i] = runif(1)
}
x
set.seed(61234)
nrep = 10
x = rep(NA, nrep)
for (i in 1:nrep){
x[i] = runif(1)
}
x
set.seed(61234)
nrep = 10
x = rep(NA, nrep)
for (i in 1:nrep){
x[i] = runif(1)
}
x
set.seed(61234)
nrep = 10
x = rep(NA, nrep)
for (i in 1:nrep){
x[i] = runif(1)
}
x
library(doParallel)
cl <- makeCluster(4)
registerDoParallel(cl)
set.seed(123)
res <- foreach(i=1:5) %dopar% { runif(1) }
set.seed(123)
res2 <- foreach(i=1:5) %dopar% { runif(1) }
stopCluster(cl)
identical(res, res2)
res
res2
library(doParallel)
cl <- makeCluster(4)
registerDoParallel(cl)
set.seed(123)
res <- foreach(i=1:5) %dopar% { runif(1) }
set.seed(123)
res2 <- foreach(i=1:5) %dopar% { runif(1) }
stopCluster(cl)
identical(res, res2)
library(doRNG)
library(doParallel)
cl <- makeCluster(4)
registerDoParallel(cl)
set.seed(123)
res <- foreach(i=1:5) %dorng% { runif(1) }
set.seed(123)
res2 <- foreach(i=1:5) %dorng% { runif(1) }
stopCluster(cl)
identical(res, res2)
library(doRNG)
library(doParallel)
cl <- makeCluster(4)
registerDoParallel(cl)
set.seed(123)
res <- foreach(i=1:5) %dorng% { runif(1) }
set.seed(123)
res2 <- foreach(i=1:5) %dorng% { runif(1) }
stopCluster(cl)
identical(res, res2)
library(doRNG)
library(doParallel)
cl <- makeCluster(4)
registerDoParallel(cl)
set.seed(123)
res <- foreach(i=1:5) %dorng% { runif(1) }
set.seed(123)
res2 <- foreach(i=1:5) %dorng% { runif(1) }
stopCluster(cl)
identical(res, res2)
0.25 * 9000
library(glmnet)
?glmnet
install.packages("postGGIR")
remove.packages("MCMCMixBimodal")
library("MCMCMixBimodal")
library("MCMCMixBimodal")
library(MCMCMixBimodal)
devtools::install_github("SrijatoBhattacharyya/MCMCMixBimodal")
library("MCMCMixBimodal")
0.2 * 45
10/45
12/45
13/45
12/45
17/45
14/25
14/45
7/(7 + 5 + 7 + 1)
14/45
5/45
8/45
15/45
(14 + 15)/45
(14 + 18)/45
13/45
14/45
15/45
18/45
8/45
11/45
2/45
3/45
devtools::install_github("minjee-kim/nhanesGraph", build_vignettes = TRUE)
library(nhanesGraph)
library(nhanesGraph)
head(nhanes_table("2008", "EPH", demographics = T, recode = T))
out = nhanes_table("2008", "EPH", demographics = T, recode = T)
?nhanes_table
out = nhanes_table("2022", "EPH", demographics = T, recode = T)
out = nhanes_table("2022", "PEH", demographics = T, recode = T)
out = nhanes_table("2008", "PEH", demographics = T, recode = T)
out = nhanes_table("2008", "EPH", demographics = F, recode = T)
head(out)
nGraph_search()
?nGraph_search
nhanes_viz(graph_type = "hist", file_name = "BPX_D", variable = "BPXSY2")
?nhanes_viz
nhanes_viz(graph_type = "his", file_name = "BPX_D", variable = "BPXSY2")
nhanes_viz(graph_type = "bla", file_name = "BPX_D", variable = "BPXSY2")
nhanes_viz(graph_type = "Hist", file_name = NULL, variable = NULL)
a
nGraph_variable()
?nGraph_variable
year_check
viz
nhanes_variable_list
devtools::install_github("jiayilei/gppcv")
library(standardGP)
library(standardGP)
library(matrixcalc)
# intent inputs
n <- 100 # number of inputs
p <- 10 # number of features
nt <- 50
set.seed(123)
X <- matrix(rnorm(n * p, 0, 0.3), n, p)
y <- matrix(rnorm(n), n, 1)
Xt <- matrix(rnorm(nt * p, 0, 0.3), nt, p)
yt <- matrix(rnorm(nt), nt, 1)
sigma2 <- 10
# first kernel, r is the Euclidean distance between two data points
k1 <- function(r) {
return(exp(-r / 1.5))
}
# second kernel, r is the Euclidean distance between two data points
k2 <- function(r) {
return(exp(-0.5 * (r / 1000)^2))
}
k <- c(k1, k2) # list of kernels
# function output
out <- gpr_seq_kernels(X, y, k, sigma2, Xt, yt)
pick_kernel(list(1), "se")
#> function(r){
#>     return (exp(-0.5 * (r/l)^2))
#>   }
#> <bytecode: 0x0000029dbefcf788>
#> <environment: 0x0000029dbefda6b8>
pick_kernel(list(2, 3), "m")
#> function (r){
#>     left <-  1 / gamma(v) / 2^(v-1)
#>     mid <- (sqrt(2*v)/ l * r)^ v
#>     right <- besselK(sqrt(2*v) * r / l, nu = v)
#>     return (left * mid * right)
#>   }
#> <bytecode: 0x0000029dbf08a438>
#> <environment: 0x0000029dbf097f30>
library(randomForest)
?randomForest
?predict.randomForest
35/40
36/40
26/40
27/40
28/40
32/40
680/8
12 * 7
9 * 3
12 * 2.4
155/12
143/12
143/9
0.2*16
155/12
setwd("~/Documents/irinagain.github.io")
(1/2)/(2/3)
n <- 100
p <- 10
## when joint has weak signal
#q1_weak <- .2
#q2_weak <- .6
#s1_weak <- .2
#s2_weak <- .45
## Pick joint strong signal setting
q1_strong <- .5
q2_strong <- .4
s1_strong <- .6
s2_strong <- .4
# My own combination to play with
#q1_strong <- .7
#q2_strong <- .3
#s1_strong <- .1
#s2_strong <- .3
# average degree of the nodes
avg_deg <- 7.5
# set parameters for model (had 0.3 first)
s <- 0.2 #0.3
k <- sqrt((1-s)^2 + s^2)
a <- matrix(0, 2, 1)
b <- matrix(0, 2, 1)
a[1, ] <- (1 - s) / k
a[2, ] <- s / k
b[1, ] <- a[2, ]
b[2, ] <- a[1, ]
thetas <- acos((2 * (1-s) * s) / k^2) # this is 0.76; pi/2 = 1.57, so this is ~45 degrees or so
# set seed here
set.seed(100000)
# Generate rank 1 joint, rank 1 individual 1, rank 1 individual 2
params <- gen_params(n = n, p = p, q1 = q1_strong, q2 = q2_strong, s1 = s1_strong, s2 = s2_strong, a = a[, 1], b = b[,1], Q=NULL, avg_deg = avg_deg,
joint_sig = 1)
source("~/Library/CloudStorage/GoogleDrive-irinagn@umich.edu/My Drive/Research/2022_Carson_NetworkCovariates/Netdecom.R")
source("~/Library/CloudStorage/GoogleDrive-irinagn@umich.edu/My Drive/Research/2022_Carson_NetworkCovariates/functions.R")
n <- 100
p <- 10
## when joint has weak signal
#q1_weak <- .2
#q2_weak <- .6
#s1_weak <- .2
#s2_weak <- .45
## Pick joint strong signal setting
q1_strong <- .5
q2_strong <- .4
s1_strong <- .6
s2_strong <- .4
# My own combination to play with
#q1_strong <- .7
#q2_strong <- .3
#s1_strong <- .1
#s2_strong <- .3
# average degree of the nodes
avg_deg <- 7.5
# set parameters for model (had 0.3 first)
s <- 0.2 #0.3
k <- sqrt((1-s)^2 + s^2)
a <- matrix(0, 2, 1)
b <- matrix(0, 2, 1)
a[1, ] <- (1 - s) / k
a[2, ] <- s / k
b[1, ] <- a[2, ]
b[2, ] <- a[1, ]
thetas <- acos((2 * (1-s) * s) / k^2) # this is 0.76; pi/2 = 1.57, so this is ~45 degrees or so
# set seed here
set.seed(100000)
# Generate rank 1 joint, rank 1 individual 1, rank 1 individual 2
params <- gen_params(n = n, p = p, q1 = q1_strong, q2 = q2_strong, s1 = s1_strong, s2 = s2_strong, a = a[, 1], b = b[,1], Q=NULL, avg_deg = avg_deg,
joint_sig = 1)
M <- params$M
R1 <- params$R1
R2 <- params$R2
# Here crossprod R1, R2 is exactly cos(theta)
crossprod(R1, R2) #0.72
cos(thetas)  #0.76
# Everything is orthogonal to joint
crossprod(M, R1)
crossprod(M, R2)
crossprod(M)
crossprod(R1)
crossprod(R2)
Z <- params$Z # This is the matrix of latent positions
W <- params$W # This is the mean of X
sum((tcrossprod(svd(W)$u[, 1:2]) - tcrossprod(cbind(M, R2)))^2) # This is zero, correct basis
P <- params$P # This is the probability network matrix
# Here Z is orthogonal but not orthonormal
crossprod(Z) # Not orthonormal though
# Here P has negative entries
min(P)
1490.88/3
install.packages("rGV")
library(rGV)
?mage
mage(x=c(rep(100, 10), rep(120, 10), 105, 85), times=seq(0, 1260, 60))
library(iglu)
library(rGV)
?mage
mage
rGV::mage
setwd("~/Documents/irinagain.github.io")
4352.97 - 2250
19450/12
94/95
9395
93/95
583+53
setwd("~/Library/CloudStorage/GoogleDrive-irinagn@umich.edu/My Drive/Research/2024_Joyce_GLOOKO/2024_Summer_Walter/Rscripts/Data Duplication Issue/Sample Code and Data")
setwd("~/Documents/irinagain.github.io")
setwd("~/Library/CloudStorage/GoogleDrive-irinagn@umich.edu/My Drive/Research/2024_Joyce_GLOOKO/2024_Summer_Walter/Rscripts/Data Duplication Issue/Sample Code and Data")
data <- read.csv("730_subject_guid.csv")
head(data)
str(data)
?read.csv
data <- read.csv("730_subject_guid.csv", colClasses = c("character", "character", "integer", "character"))
str(data)
table(data$id)
# Read another dataset
data_small <- read.csv("data_2_subject.csv")
library(iglu)
plot_glu(data_small)
library(iglu)
# Time conversion
data_small$time <- as.POSIXct(data_small$time)
plot_glu(data_small)
install.packages("iglu")
library(iglu)
plot_glu(data_small)
# This has many subjects and dates per subject
ns = unique(data$id)
library(lubridate)
library(dplyr)
library(purrr)
#isolating data of interest (dates and subject)
visit_date <- ymd_hms(visit_date)
# Try processing on the first subject to start
sub_id <- data$id[1]
visit_date <- tail((data %>% filter(id == sub_id) %>% arrange(time))$time, n = 1)
dt0 = 5
acceptable_diff = dt0 - 0.5 # this will be 4.5 minutes
#isolating data of interest (dates and subject)
visit_date <- ymd_hms(visit_date)
visit_interval <- interval(start = visit_date - days(n_days), end = visit_date)
n_days = 14
visit_interval <- interval(start = visit_date - days(n_days), end = visit_date)
data <- data %>%
filter(time %within% visit_interval & id == sub_id) %>%
arrange(time)
20 * 12
150.12
150/12
300/12
300/20
300/18
300/17
300/15
20/12
24 + 4
28-15
2 * 300 * 100 * 1.05
14258*(1.03)^3
15580 + 23528
40 + 80
314/4
314/5
setwd("~/Documents/irinagain.github.io")
60 * 2 * 300 * 1.05
60 * 2 * 300 * 1.1
60 * 2 * 300 * 1.07
60 * 2 * 300 * 1.08
+ 700
60 * 2 * 300 * 1.08 + 700
60 * 2 * 300 * 1.09 + 700
60 * 2 * 300 * 1.1 + 700
60 * 2 * 300 * 1.1
50 * 3 * 300 * 1.1
50 * 2 * 300 * 1.1
50 * 3 * 300 * 1.1
